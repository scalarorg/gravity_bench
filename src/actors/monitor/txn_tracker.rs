use std::cmp::Ordering;
use std::collections::{BTreeSet, HashMap, HashSet, VecDeque};
use std::sync::Arc;
use std::time::{Duration, Instant};

use alloy::consensus::Account;
use alloy::primitives::TxHash;
use comfy_table::{presets::UTF8_FULL, Cell, Table};
use tracing::{debug, error, warn};

use crate::actors::monitor::SubmissionResult;
use crate::eth::EthHttpCli;
use crate::txn_plan::{PlanId, TxnMetadata};

use super::UpdateSubmissionResult;

const SAMPLING_SIZE: usize = 10; // Define sampling size
const TXN_TIMEOUT: Duration = Duration::from_secs(600); // 10 minutes timeout
const TPS_WINDOW: Duration = Duration::from_secs(17);

// Backpressure configuration
const MAX_PENDING_TXNS: usize = 200_000;
const BACKPRESSURE_RESUME_THRESHOLD: usize = 160_000; // 80% of max

// Retry configuration
const RETRY_TIMEOUT: Duration = Duration::from_secs(120); // Retry if stuck for 120s
const MAX_RETRY_BATCH: usize = 10_000; // Max transactions to retry per batch

/// Format large numbers with appropriate suffixes (K, M, B)
fn format_large_number(num: u64) -> String {
    if num >= 1_000_000_000 {
        format!("{:.1}B", num as f64 / 1_000_000_000.0)
    } else if num >= 1_000_000 {
        format!("{:.1}M", num as f64 / 1_000_000.0)
    } else if num >= 10_000 {
        format!("{:.1}K", num as f64 / 1_000.0)
    } else {
        num.to_string()
    }
}

/// Transaction and plan lifecycle tracker
pub struct TxnTracker {
    /// Plan tracker
    plan_trackers: HashMap<PlanId, PlanTracker>,
    /// Time-sorted set of in-flight transactions
    /// Using BTreeSet allows automatic sorting based on our implemented Ord
    pending_txns: BTreeSet<PendingTxInfo>,
    /// RPC client mapping
    clients: HashMap<String, Arc<EthHttpCli>>,
    /// Timestamps of resolved transactions for TPS calculation
    resolved_txn_timestamps: VecDeque<Instant>,
    /// Latency of resolved transactions
    latencies: VecDeque<Duration>,
    total_produced_transactions: u64,
    total_resolved_transactions: u64,
    total_failed_submissions: u64,
    total_failed_executions: u64,
    last_completed_plan: Option<(PlanId, PlanTracker)>,
    producer_ready_accounts: u64,
    producer_sending_txns: u64,
    mempool_pending: u64,
    mempool_queued: u64,
    /// Track if producer was paused due to pending txn limit
    producer_paused_by_pending: bool,
}

/// Tracking status of a single transaction plan
#[derive(Debug, Clone)]
struct PlanTracker {
    /// Total number of transactions in the plan
    produce_transactions: usize,
    /// Number of transactions that have reached final state
    resolved_transactions: u64,
    /// Number of consumed transactions
    consumed_transactions: u64,
    /// Number of failed submissions
    failed_submissions: u64,
    /// Number of failed executions (reverted)
    failed_executions: u64,

    plan_produced: bool,

    plan_name: String,
}

/// Detailed information of in-flight transactions
/// Added Eq, Ord and other Trait implementations to enable storage and sorting in BTreeSet
#[derive(Debug, Clone)]
pub(crate) struct PendingTxInfo {
    tx_hash: TxHash,
    metadata: Arc<TxnMetadata>,
    rpc_url: String,
    submit_time: Instant,
    /// Signed transaction bytes for retry support
    signed_bytes: Arc<Vec<u8>>,
}

//--- Core implementation required for BTreeSet sorting ---//

impl PartialEq for PendingTxInfo {
    fn eq(&self, other: &Self) -> bool {
        self.submit_time == other.submit_time && self.tx_hash == other.tx_hash
    }
}
impl Eq for PendingTxInfo {}

impl PartialOrd for PendingTxInfo {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for PendingTxInfo {
    /// Sorting rules:
    /// 1. Primarily sorted by submission time (`submit_time`) in ascending order
    /// 2. If submission times are the same, sort by transaction hash (`tx_hash`) to ensure uniqueness
    fn cmp(&self, other: &Self) -> Ordering {
        self.submit_time
            .cmp(&other.submit_time)
            .then_with(|| self.tx_hash.cmp(&other.tx_hash))
    }
}

/// Plan completion status
#[derive(Debug)]
pub enum PlanStatus {
    Completed,
    Failed { reason: String },
    InProgress,
}

/// Backpressure action to control producer
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum BackpressureAction {
    Pause,
    Resume,
    None,
}

/// Transaction ready for retry
#[derive(Debug, Clone)]
pub struct RetryTxnInfo {
    pub signed_bytes: Arc<Vec<u8>>,
    pub metadata: Arc<TxnMetadata>,
}

impl TxnTracker {
    /// Create new transaction tracker
    pub fn new(clients: Vec<Arc<EthHttpCli>>) -> Self {
        let mut client_map = HashMap::new();
        for client in clients {
            let rpc_url = client.rpc().as_ref().clone();
            client_map.insert(rpc_url, client);
        }

        Self {
            plan_trackers: HashMap::new(),
            pending_txns: BTreeSet::new(),
            clients: client_map,
            resolved_txn_timestamps: VecDeque::new(),
            latencies: VecDeque::new(),
            total_produced_transactions: 0,
            total_resolved_transactions: 0,
            total_failed_submissions: 0,
            total_failed_executions: 0,
            last_completed_plan: None,
            producer_ready_accounts: 0,
            producer_sending_txns: 0,
            mempool_pending: 0,
            mempool_queued: 0,
            producer_paused_by_pending: false,
        }
    }

    pub fn update_producer_stats(&mut self, ready_accounts: u64, sending_txns: u64) {
        self.producer_ready_accounts = ready_accounts;
        self.producer_sending_txns = sending_txns;
    }

    pub fn update_mempool_stats(&mut self, pending: u64, queued: u64) {
        self.mempool_pending = pending;
        self.mempool_queued = queued;
    }

    /// Check if backpressure should be applied based on pending txn count
    pub fn check_pending_backpressure(&mut self) -> BackpressureAction {
        let current = self.pending_txns.len();
        if current >= MAX_PENDING_TXNS && !self.producer_paused_by_pending {
            self.producer_paused_by_pending = true;
            warn!(
                "Pending txns {} >= {}, pausing producer",
                current, MAX_PENDING_TXNS
            );
            BackpressureAction::Pause
        } else if current < BACKPRESSURE_RESUME_THRESHOLD && self.producer_paused_by_pending {
            self.producer_paused_by_pending = false;
            tracing::info!(
                "Pending txns {} < {}, resuming producer",
                current,
                BACKPRESSURE_RESUME_THRESHOLD
            );
            BackpressureAction::Resume
        } else {
            BackpressureAction::None
        }
    }

    pub fn handler_produce_txns(&mut self, plan_id: PlanId, count: usize) {
        if let Some(tracker) = self.plan_trackers.get_mut(&plan_id) {
            tracker.produce_transactions += count;
            self.total_produced_transactions += count as u64;
        }
    }

    pub fn handle_plan_produced(&mut self, plan_id: PlanId, _count: usize) {
        if let Some(tracker) = self.plan_trackers.get_mut(&plan_id) {
            tracker.plan_produced = true;
        }
    }

    /// Register new plan (no changes)
    pub fn register_plan(&mut self, plan_id: PlanId, plan_name: String) {
        debug!("Plan registered: plan_id={}", plan_id);
        let tracker = PlanTracker {
            produce_transactions: 0,
            resolved_transactions: 0,
            consumed_transactions: 0,
            failed_submissions: 0,
            failed_executions: 0,
            plan_produced: false,
            plan_name,
        };
        self.plan_trackers.insert(plan_id, tracker);
    }

    /// Handle transaction submission result
    pub fn handle_submission_result(&mut self, msg: &UpdateSubmissionResult) {
        let plan_id = &msg.metadata.plan_id;
        if !self.plan_trackers.contains_key(plan_id) {
            warn!("Plan not found: plan_id={}", plan_id);
            return;
        }
        let plan_tracker = self.plan_trackers.get_mut(plan_id).unwrap();

        plan_tracker.consumed_transactions += 1;
        match msg.result.as_ref() {
            SubmissionResult::Success(tx_hash) => {
                debug!(
                    "Transaction submitted successfully: plan_id={}, tx_hash={:?}, rpc_url={}",
                    plan_id, tx_hash, msg.rpc_url
                );

                let pending_info = PendingTxInfo {
                    tx_hash: *tx_hash,
                    metadata: msg.metadata.clone(),
                    rpc_url: msg.rpc_url.clone(),
                    submit_time: Instant::now(),
                    signed_bytes: msg.signed_bytes.clone(),
                };

                // Insert transaction into the global, time-sorted BTreeSet
                self.pending_txns.insert(pending_info);
            }
            SubmissionResult::NonceTooLow {
                tx_hash,
                expect_nonce,
                actual_nonce,
                from_account,
            } => {
                let pending_info = PendingTxInfo {
                    tx_hash: *tx_hash,
                    metadata: msg.metadata.clone(),
                    rpc_url: msg.rpc_url.clone(),
                    submit_time: Instant::now(),
                    signed_bytes: msg.signed_bytes.clone(),
                };
                self.pending_txns.insert(pending_info);
                debug!(
                    "Transaction submission failed because nonce is too low: account={:?}, expect_nonce={}, actual_nonce={}, tx_hash={:?}",
                    from_account, expect_nonce, actual_nonce, tx_hash
                );
            }
            e => {
                warn!(
                    "Transaction submission failed: plan_id={}, error={:?}",
                    plan_id, e
                );
                if let Some(tracker) = self.plan_trackers.get_mut(plan_id) {
                    tracker.resolved_transactions += 1;
                    tracker.failed_submissions += 1;
                    self.total_failed_submissions += 1;
                    self.resolved_txn_timestamps.push_back(Instant::now());
                    self.total_resolved_transactions += 1;
                }
            }
        }
    }

    /// Check if plan is completed (no changes)
    pub fn check_plan_completion(&mut self, plan_id: &PlanId) -> PlanStatus {
        let mut status = PlanStatus::InProgress;
        if let Some(tracker) = self.plan_trackers.get(plan_id) {
            if tracker.plan_produced {
                if tracker.resolved_transactions as usize >= tracker.produce_transactions {
                    let has_failures =
                        tracker.failed_submissions > 0 || tracker.failed_executions > 0;
                    status = if has_failures {
                        let reason = format!(
                            "Plan failed: {} submission failures, {} execution failures",
                            tracker.failed_submissions, tracker.failed_executions
                        );
                        warn!("Plan {} failed: {}", tracker.plan_name, reason);
                        PlanStatus::Failed { reason }
                    } else {
                        debug!("Plan {} completed successfully", plan_id);
                        PlanStatus::Completed
                    };
                }
            }
        }
        if let PlanStatus::Completed = status {
            if let Some(completed_tracker) = self.plan_trackers.remove(plan_id) {
                self.last_completed_plan = Some((plan_id.clone(), completed_tracker));
            }
        }
        status
    }

    /// Get all active plan IDs being tracked (no changes)
    pub fn get_active_plan_ids(&self) -> Vec<PlanId> {
        self.plan_trackers.keys().cloned().collect()
    }

    pub fn perform_sampling_check(
        &mut self,
    ) -> Vec<
        impl std::future::Future<
            Output = (
                PendingTxInfo,
                Result<Option<Account>, anyhow::Error>,
                Result<Option<alloy::rpc::types::TransactionReceipt>, anyhow::Error>,
            ),
        >,
    > {
        let total_pending = self.pending_txns.len();
        if total_pending == 0 {
            return Vec::new();
        }

        let mut samples = BTreeSet::new(); // Use BTreeSet to avoid duplicates if indices overlap
        let mut tasks = Vec::new();

        // --- Core sampling logic ---
        if total_pending <= SAMPLING_SIZE {
            // If total is less than sampling size, check all
            samples.extend(self.pending_txns.iter().cloned());
        } else {
            // Select samples at fixed intervals in the queue
            // For example, with 1000 txns and 10 samples, take one every 100
            let step = total_pending / SAMPLING_SIZE;
            for i in 0..SAMPLING_SIZE {
                let index = i * step;
                if let Some(txn_info) = self.pending_txns.iter().nth(index) {
                    samples.insert(txn_info.clone());
                }
            }
            // Always include the oldest one as it's most critical
            if let Some(oldest) = self.pending_txns.iter().next() {
                samples.insert(oldest.clone());
            }
        }

        for pending_info in samples {
            if let Some(client) = self.clients.get(&pending_info.rpc_url) {
                let client = client.clone();
                let task_info = pending_info.clone();

                let task = async move {
                    let result = client.get_transaction_receipt(task_info.tx_hash).await;
                    let account = client
                        .get_account(*task_info.metadata.from_account.as_ref())
                        .await;
                    tracing::debug!(
                        "checked tx_hash={:?} result={:?}",
                        task_info.tx_hash,
                        result
                    );
                    (task_info, account, result)
                };
                tasks.push(task);
            } else {
                warn!("No client found for RPC URL: {}", pending_info.rpc_url);
            }
        }

        tasks
    }

    pub fn handle_receipt_result(
        &mut self,
        results: Vec<(
            PendingTxInfo,
            Result<Option<Account>, anyhow::Error>,
            Result<Option<alloy::rpc::types::TransactionReceipt>, anyhow::Error>,
        )>,
    ) -> Vec<RetryTxnInfo> {
        let mut successful_txns = Vec::new();
        let mut failed_txns = Vec::new(); // Including Pending, Timeout, Error
        let mut retry_queue = Vec::new();

        // 1. Categorize results
        for (info, account, result) in results {
            match result {
                Ok(Some(receipt)) => {
                    // Transaction successfully confirmed
                    self.pending_txns.remove(&info);
                    successful_txns.push((info, receipt.status()));
                }
                Ok(None) => {
                    // Transaction still pending
                    if let Ok(Some(account)) = account {
                        if account.nonce > info.metadata.nonce {
                            successful_txns.push((info, true));
                        }
                    } else {
                        // Account doesn't exist or error getting account - transaction still pending
                        // Don't mark as failed, just continue monitoring
                    }
                }
                Err(e) => {
                    // RPC query failed
                    warn!(
                        "Failed to get receipt for tx_hash={:?}: {}",
                        info.tx_hash, e
                    );
                    failed_txns.push(info);
                }
            }
        }

        if !failed_txns.is_empty() {
            debug!(
                "Failed to get receipt for {} transactions",
                failed_txns.len()
            );
        }

        let successful_txns_hash = successful_txns
            .iter()
            .map(|(info, _)| info.tx_hash)
            .collect::<HashSet<_>>();

        // 2. If there are successful transactions, calculate median time and clean up
        if !successful_txns.is_empty() {
            // Create a temporary TxnInfo for BTreeSet split_off
            // TxHash is not important as sorting is mainly based on time
            let split_info = successful_txns[successful_txns.len() - 1].0.clone();

            // Use split_off to efficiently split BTreeSet
            // `cleared_txns` contains all transactions with time <= median_time
            let cleared_txns = self.pending_txns.split_off(&split_info);

            // Process transactions that were batch cleaned
            for cleared_info in self.pending_txns.iter() {
                if successful_txns_hash.contains(&cleared_info.tx_hash) {
                    continue;
                }

                let latency = cleared_info.submit_time.elapsed();
                self.latencies.push_back(latency);
                if self.latencies.len() > 1000 {
                    self.latencies.pop_front();
                }

                if let Some(plan_tracker) =
                    self.plan_trackers.get_mut(&cleared_info.metadata.plan_id)
                {
                    plan_tracker.resolved_transactions += 1;
                    self.resolved_txn_timestamps.push_back(Instant::now());
                    self.total_resolved_transactions += 1;
                }
            }

            // Update main queue with the remaining part (newer transactions)
            self.pending_txns = cleared_txns;
        }

        // 3. Process the confirmed transactions from this sampling
        for (info, receipt_status) in successful_txns {
            let latency = info.submit_time.elapsed();
            self.latencies.push_back(latency);
            if self.latencies.len() > 1000 {
                self.latencies.pop_front();
            }

            if let Some(plan_tracker) = self.plan_trackers.get_mut(&info.metadata.plan_id) {
                plan_tracker.resolved_transactions += 1;
                self.resolved_txn_timestamps.push_back(Instant::now());
                self.total_resolved_transactions += 1;
                if !receipt_status {
                    plan_tracker.failed_executions += 1;
                    self.total_failed_executions += 1;
                    warn!(
                        "Transaction reverted: plan_id={}, tx_hash={:?}",
                        info.metadata.plan_id, info.tx_hash
                    );
                }
            }
        }

        // 4. Process failed or still pending transactions
        // Find the youngest (latest submit_time) transaction that has timed out (>120s)
        // Then retry ALL transactions older than or equal to it (up to MAX_RETRY_BATCH)
        let retry_cutoff: Option<PendingTxInfo> = failed_txns
            .iter()
            .filter(|info| info.submit_time.elapsed() > RETRY_TIMEOUT)
            .max_by_key(|info| info.submit_time)
            .cloned();

        if let Some(cutoff) = retry_cutoff {
            // Collect all transactions older than or equal to the cutoff
            let to_process: Vec<_> = self
                .pending_txns
                .iter()
                .take_while(|info| info.submit_time <= cutoff.submit_time)
                .take(MAX_RETRY_BATCH)
                .cloned()
                .collect();

            let batch_size = to_process.len();
            if batch_size > 0 {
                warn!(
                    "Detected stuck transactions, processing interval of {} txns (cutoff tx={}, stuck for {}s)",
                    batch_size,
                    cutoff.tx_hash,
                    cutoff.submit_time.elapsed().as_secs()
                );
            }

            for info in to_process {
                self.pending_txns.remove(&info);

                if info.submit_time.elapsed() > TXN_TIMEOUT {
                    // Transaction has completely timed out (10 min), mark as failed
                    error!(
                        "Transaction completely timed out: plan_id={}, tx_hash={:?}",
                        info.metadata.plan_id, info.tx_hash
                    );
                    if let Some(plan_tracker) = self.plan_trackers.get_mut(&info.metadata.plan_id) {
                        plan_tracker.resolved_transactions += 1;
                        plan_tracker.failed_executions += 1;
                        self.total_failed_executions += 1;
                        self.resolved_txn_timestamps.push_back(Instant::now());
                        self.total_resolved_transactions += 1;
                    }
                } else {
                    // Queue for retry (and remove from pending to avoid duplicate retry)
                    retry_queue.push(RetryTxnInfo {
                        signed_bytes: info.signed_bytes.clone(),
                        metadata: info.metadata.clone(),
                    });
                }
            }
        } else {
            // No timed-out transactions in samples, just re-insert failed ones
            for info in failed_txns {
                if info.submit_time.elapsed() > TXN_TIMEOUT {
                    error!(
                        "Transaction completely timed out: plan_id={}, tx_hash={:?}",
                        info.metadata.plan_id, info.tx_hash
                    );
                    self.pending_txns.remove(&info);
                    if let Some(plan_tracker) = self.plan_trackers.get_mut(&info.metadata.plan_id) {
                        plan_tracker.resolved_transactions += 1;
                        plan_tracker.failed_executions += 1;
                        self.total_failed_executions += 1;
                        self.resolved_txn_timestamps.push_back(Instant::now());
                        self.total_resolved_transactions += 1;
                    }
                }
                // If not timed out, they stay in pending_txns (already there from earlier)
            }
        }

        retry_queue
    }

    pub fn log_stats(&mut self) {
        // Update TPS window by removing timestamps older than 30 seconds
        let now = Instant::now();
        let window_start = now - TPS_WINDOW;
        while let Some(ts) = self.resolved_txn_timestamps.front() {
            if *ts < window_start {
                self.resolved_txn_timestamps.pop_front();
            } else {
                break;
            }
        }

        // Calculate TPS
        let tps = self.resolved_txn_timestamps.len() as f64 / TPS_WINDOW.as_secs_f64();

        // Calculate latency stats
        let (avg_latency, _min_latency, _max_latency) = if !self.latencies.is_empty() {
            let sum: Duration = self.latencies.iter().sum();
            let avg = sum / self.latencies.len() as u32;
            let min = *self.latencies.iter().min().unwrap();
            let max = *self.latencies.iter().max().unwrap();
            (avg, min, max)
        } else {
            (
                Duration::from_secs(0),
                Duration::from_secs(0),
                Duration::from_secs(0),
            )
        };

        // Calculate success rate
        let success_rate = if self.total_produced_transactions > 0 {
            self.total_resolved_transactions as f64 / self.total_produced_transactions as f64
                * 100.0
        } else {
            0.0
        };

        // Count plan states
        let mut produced_plans = 0u64;
        let mut not_produced_plans = 0u64;
        let mut completed_plans = 0u64;
        let mut in_progress_plans = 0u64;
        let mut timed_out_txns = 0u64;

        for (_plan_id, tracker) in &self.plan_trackers {
            if tracker.plan_produced {
                produced_plans += 1;
                if tracker.resolved_transactions as usize >= tracker.produce_transactions {
                    completed_plans += 1;
                } else {
                    in_progress_plans += 1;
                }
            } else {
                not_produced_plans += 1;
            }
        }

        // Count timed out pending txns (those waiting > RETRY_TIMEOUT)
        for pending_info in &self.pending_txns {
            if pending_info.submit_time.elapsed() > RETRY_TIMEOUT {
                timed_out_txns += 1;
            }
        }

        let pending_txns = self.pending_txns.len() as u64;

        let mut table = Table::new();
        table.load_preset(UTF8_FULL);

        // Set table header - summary statistics
        table.set_header(vec!["Metric", "Value", "Metric", "Value"]);

        // Row 1: Txn progress and TPS
        table.add_row(vec![
            Cell::new("Progress"),
            Cell::new(&format!(
                "{}/{}",
                format_large_number(self.total_resolved_transactions),
                format_large_number(self.total_produced_transactions)
            )),
            Cell::new("TPS"),
            Cell::new(&format!("{:.1}", tps)),
        ]);

        // Row 2: Pending txns and latency
        table.add_row(vec![
            Cell::new("Pending Txns"),
            Cell::new(&format_large_number(pending_txns)),
            Cell::new("Avg Latency"),
            Cell::new(&format!("{:.1}s", avg_latency.as_secs_f64())),
        ]);

        // Row 3: Timed out txns and success rate
        table.add_row(vec![
            Cell::new("Timed Out Txns"),
            Cell::new(&format_large_number(timed_out_txns)),
            Cell::new("Success%"),
            Cell::new(&format!("{:.1}%", success_rate)),
        ]);

        // Row 4: Produced plans and not produced plans
        table.add_row(vec![
            Cell::new("Produced Plans"),
            Cell::new(&format_large_number(produced_plans)),
            Cell::new("Not Produced"),
            Cell::new(&format_large_number(not_produced_plans)),
        ]);

        // Row 5: Completed plans and in progress plans
        table.add_row(vec![
            Cell::new("Completed Plans"),
            Cell::new(&format_large_number(completed_plans)),
            Cell::new("In Progress"),
            Cell::new(&format_large_number(in_progress_plans)),
        ]);

        // Row 6: Send failures and execution failures
        table.add_row(vec![
            Cell::new("Send Failures"),
            Cell::new(&format_large_number(self.total_failed_submissions)),
            Cell::new("Exec Failures"),
            Cell::new(&format_large_number(self.total_failed_executions)),
        ]);

        // Row 7: Mempool stats
        table.add_row(vec![
            Cell::new("Pool Pending"),
            Cell::new(&format_large_number(self.mempool_pending)),
            Cell::new("Pool Queued"),
            Cell::new(&format_large_number(self.mempool_queued)),
        ]);

        // Row 8: Producer stats
        table.add_row(vec![
            Cell::new("Ready Accounts"),
            Cell::new(&format_large_number(self.producer_ready_accounts)),
            Cell::new("Processing"),
            Cell::new(&format_large_number(self.producer_sending_txns)),
        ]);

        tracing::info!("\n{}", table);
    }
}
